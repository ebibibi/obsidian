- [[Transformer]]は効率的な総当たりを可能にした
- [[Transformer]]以前はGPUを自然言語処理に効率的に適用できていなかった。前後に行きつ戻りつしながら意味や指示語、代名詞等を扱う必要があったため、並列計算と相性が良くなかった。
- 自然言語では[[RNN]]と呼ばれる時系列を扱うための構造を用いるのが普通であり、順序による依存関係を考慮する必要があるため、複数の依存関係間での調節(待ち)が発生し、並列化しにくくなっていた。
- これを打破する機構が[[Attentoin機構]]。文章中の単語に粗油店を充てることでその単語と関係するあらゆるつながりを文章中で見つける方法。
- メモリ上の文章はあたかも画像のように扱えるため、並列計算を当てはめることができるようになった。
- さらに、全結合によりテキストの相互参照による重み関係のみを学習すればよいため、一般的な教師あり学習が必ずしも必要ではなくなった。
	- これ以前は自然言語の学習は簡単に評価できないため、学習データの一部を教師データとしその中でテストを行い問題と正解を教える必要があった。この問題と正解のペアを作成するコストが非常にたかく