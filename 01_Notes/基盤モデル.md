- 大量・多様なデータから高い汎化性能を獲得したAI
- 2021年にスタンフォード大学のワーキンググループによって命名された。
- 基盤モデルは単体で多様なタスクを解くことができる。
- 大量かつ多様なデータで訓練され、多様な下流タスクに適応（[[ファインチューニング]]など）できるモデル

## 具体例
- [[BERT]]
- [[GPT-3]]
- [[CLIP]]

## 特徴
- 従来のAI
	- 解きたいタスクごと人力のラベリングを行いながらデータセットを用意し、適切なアーキテクチャのモデルを設計および訓練する必要がある。そのため、開発、運用コストが高い。
	- 目的のタスクに特化されたモデルは分布外データに対する予測性能が著しく低くなってしまう。
- 基盤モデル
	- タスクごとに集めるデータは少量で十分。
	- 目的のタスクに特化させないため、分布外データに対する予測性能も良い。
	- [[GPT-3]]以降はタスクの内容を自然言語で記述し、それに答える