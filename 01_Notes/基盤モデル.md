- 大量・多様なデータから高い汎化性能を獲得したAI
- 2021年にスタンフォード大学のワーキンググループによって命名された。
- 基盤モデルは単体で多様なタスクを解くことができる。
- 大量かつ多様なデータで訓練され、多様な下流タスクに適応（[[ファインチューニング]]など）できるモデル

## 具体例
- [[BERT]]
- [[GPT-3]]
- [[CLIP]]
- [[PaLM]]

## 特徴
- 従来のAI
	- 解きたいタスクごと人力のラベリングを行いながらデータセットを用意し、適切なアーキテクチャのモデルを設計および訓練する必要がある。そのため、開発、運用コストが高い。
	- 目的のタスクに特化されたモデルは分布外データに対する予測性能が著しく低くなってしまう。
- 基盤モデル
	- タスクごとに集めるデータは少量で十分。
	- 目的のタスクに特化させないため、分布外データに対する予測性能も良い。
	- [[GPT-3]]以降はタスクの内容を自然言語で記述し、それに応えることが可能なモデルが増えている。

## スケーリング則
- 基盤モデルは下記の3つのパラメーターを大きくすればするほど性能が向上するといわれている。
	- 訓練ステップ数
	- データセットのサイズ
	- パラメータ数
- まだどこまで性能が向上させられるのかはわかっていない。
- このことから、「タスクごとに専用のモデルを開発する」のではなく「最強のモデルを1つ作って使いまわす」方向に移りつつある。

## 明示的に教えていないのに学習する
- 基盤モデルは膨大なパラメーターと膨大な学習データーにより明示的に学習させていないものごとを学習可能となった。
- 例えばテキストデータのみを学習させることで、四則演算を学習した。

## [[Transformer]]
- 膨大な学習パラメータの効率来な学習および、機能の発現を可能にしたのは[[BERT]]で注目された技術である[[Transformer]]。

## 種類
- 基盤モデルは「言語」を扱うものと「視覚・言語」を扱うものとに大きく区分される。
- 「視覚・言語」を扱うモデルは入力および出力がどちらに対応しているかによって分類することができる。

## [[プロンプトエンジニアリング]]
- [[GPT-3]]のようなモデルに対しては、予測精度を上げるにはモデルに与える説明文や出力例が需要となる。モデルにとってわかりやすい説明文、入出力例を与える工夫を[[プロンプトエンジニアリング]]と呼ぶ。
- [[PaLM]]では因果関係や常識の理解を必要とする難しいタスクも解けるようになった。さらに、入出力の例にその思考過程を含めることでそれまで解けなかった問題を解けるようになることも発見され、注目された。

## 課題
- データセットに含まれる有害な表現や偏見をそのまま学習、出力してしまう。
- 少数の声は無視されてしまう。(学習データセット内に少数しか含まれないため結果に結果に反映されにくい)
- 悪意を持った人の手に渡るとフェイクニュース等で世論を操作する等の事が容易にできてしまう。
- 学習のために膨大なエネルギーを消費する。
- 研究が資金力のある一部の機関による寡占状態となってしまっている。
- モデルやソースコードを公開しない場合も多い。