- 大量・多様なデータから高い汎化性能を獲得したAI
- 2021年にスタンフォード大学のワーキンググループによって命名された。
- 基盤モデルは単体で多様なタスクを解くことができる。
- 大量かつ多様なデータで訓練され、多様な下流タスクに適応（[[ファインチューニング]]など）できるモデル

## 具体例
- [[BERT]]
- [[GPT-3]]
- [[CLIP]]
- [[PaLM]]

## 特徴
- 従来のAI
	- 解きたいタスクごと人力のラベリングを行いながらデータセットを用意し、適切なアーキテクチャのモデルを設計および訓練する必要がある。そのため、開発、運用コストが高い。
	- 目的のタスクに特化されたモデルは分布外データに対する予測性能が著しく低くなってしまう。
- 基盤モデル
	- タスクごとに集めるデータは少量で十分。
	- 目的のタスクに特化させないため、分布外データに対する予測性能も良い。
	- [[GPT-3]]以降はタスクの内容を自然言語で記述し、それに応えることが可能なモデルが増えている。

## スケーリング則
- 基盤モデルは下記の3つのパラメーターを大きくすればするほど性能が向上するといわれている。
	- 訓練ステップ数
	- データセットのサイズ
	- パラメータ数
- まだどこまで性能が向上させられるのかはわかっていない。
- このことから、「タスクごとに専用のモデルを開発する」のではなく「最強のモデルを1つ作って使いまわす」方向に移りつつある。

## 種類
- 基盤モデルは「言語」を扱うものと「視覚・言語」を扱うものとに大きく区分される。

## プロンプトエンジニアリング