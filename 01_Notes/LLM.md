## LLMとは
- 大規模言語モデル(Large Language Model)
- 大量のテキストデータを使ってトレーニングされた自然言語処理のモデル。
- LLMを[[ファインチューニング]]することで具体的なタスク(テキスト分類、感情分析、情報抽出、文章要約、テキスト生成、質問応答)などの[[自然言語処理(NLP:Natural Language Processing)]]タスクに応用する。

## 代表例
- Google
	- [[BERT]] 2018年
- OpenAI
	- [[GPT-3]] 2020年
	- [[GPT-3.5]] 2022年
		- [[ChatGPT]]は[[GPT-3.5]]をチャット(対話)向けにファインチューニングしたもの。LLMの応用例の1つ。

## 学習ソース
- 明確な基準や決まった学習ソースがあるわけではない。
- [[BERT]]は28億語のWikipediaデータと8億語のGoogle BookCorpusで合計33億語のデータからトレーニングされている。
- [[GPT-3]]は45TBのデータ(合計4990億トークン)からトレーニングされている。

## パラメータ数
- [[LLM]]は[[ニューラルネットワーク]]に含まれるパラメーターの数も多い。
- [[BERT]] 3億4千万パラメーター
- [[GPT-3]] 1750億パラメーター
- [[GPT-3.5]] 3550億パラメーター

## [[基盤モデル]] との関係
- [[GPT-3]]や[[GPT-3.5]]は[[基盤モデル]]であり、同時に[[LLM]]でもある。
- テキスト以外のデータを使ってトレーニングした[[基盤モデル]]は大規模言語モデルではない。