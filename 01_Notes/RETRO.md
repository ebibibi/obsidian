- DeepMindの大規模自然言語モデル
- 直接学習データセットそのものを検索することで、より小さなモデルサイズで性能を維持することに成功
- 同じ数のパラメータを持つ標準的な[[Transformer:]]ベースモデルと比較して大幅な性能向上を実現
- モデル内で本来持つべきパラメータの一部を外部のデータベースにアウトソーシングする
- RETRO では、英語、スペイン語、ドイツ語、フランス語、ロシア語、中国語、スワヒリ 語、ウルドゥー語を含む 10 言語のテキストを含むニュース記事、ウィキペディアのテキス ト、書籍、GitHub のテキストで構成されるデータセットでモデルを学習する。
- 